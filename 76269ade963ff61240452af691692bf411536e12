{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "e061aa68_c8614058",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-17T13:02:40Z",
      "side": 1,
      "message": "I don\u0027t believe this is correct. Unlocks need to use release operations so strictly speaking, what we need here is ldxrb to load byte exclusively. Then we need a store release to prevent operations in the critical section to get reordered after bit_unlcock.",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "47984a88_7d4822ee",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000105
      },
      "writtenOn": "2024-03-18T13:00:10Z",
      "side": 1,
      "message": "Why don\u0027t we need Load-Acquire to ensure the required ordering as in bit_lock()?\nDo you mean by \"store release\" \"stlrb\" instruction? Shouldn\u0027t load/store exclusively come in pairs?",
      "parentUuid": "e061aa68_c8614058",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "11cedf3f_ba134e80",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-19T03:39:55Z",
      "side": 1,
      "message": "yes, we do, and we are doing that in line 87. ldaxrb is a load acquire. yes, I mean stlrb. we don\u0027t need exclusive access on the store during unlock, since it is a just a write, which should already be single copy atomic and the lock release/unlock is not an atomic read modify write like in the acquiring of lock.",
      "parentUuid": "47984a88_7d4822ee",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "44fe7167_a8dd3a5d",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2024-03-20T09:48:55Z",
      "side": 1,
      "message": "There is difference to bit lock from regular spin lock. For regular spin lock , once it is \"locked\", then there is a single owner holding the lock and only the owner can \"unlock\" it. Hence a normal str instruction will suffice for unlock.\n\nIn bit lock, we are overlaying 8 locks on a single byte. So even when a bit is locked, this byte is not owned by a single owner. It is potentially shared with 7 other owners who are racing to \"lock\" the other bits of the byte. Hence the unlock now need to perform a exclusive load-acquire store-release semantics to ensure that when the bit is unlocked, it does not inadvertently overwrite values set by other owners.\n\nSemantically a bit lock and unlock has the following behavior:\n  * locking: acquire a byte lock to set a bit.\n  * unlocking : acquire a byte lock to clear a bit.",
      "parentUuid": "11cedf3f_ba134e80",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ce84a637_adfd3ed0",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-20T10:29:08Z",
      "side": 1,
      "message": "good point.\n\nI think semantically what we need is:\nlocking: load-acquire exclusive byte, set bit, store exclusive byte.\nunlocking: load exclusive byte, clear bit, store-release exclusive byte.\n\nI think semantically you only need atomic bit set and clear, and acquire and release semantics at the start and end of the critical section respectively.\n\nLet me know what you think.",
      "parentUuid": "44fe7167_a8dd3a5d",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8f09c5ea_77775b23",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2024-03-20T11:05:33Z",
      "side": 1,
      "message": "OK, since we have a read-modify-write sequence for the lock/unlock, we still need to ensure that the access to modify the byte was done while the CPU had the exclusive access to the byte I think. Just atomicity and ordering may not be enough perhaps.\n\nIt is difficult to prove that a lock works unless we can formally prove the scheme holds good (using TLA or litmus tests). Hence the approach taken here is to try to use proven exclusive access sequence so as to avoid doing further verification.\n\nYour scheme may work, if you can send write down the asm sequence you have in mind, we can analyze it and we can take it from there.",
      "parentUuid": "ce84a637_adfd3ed0",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "08a9e8c4_13f5cdf3",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-20T11:38:37Z",
      "side": 1,
      "message": "asm sequence for lock is good from my pov. that is what the code does today.\nfor unlock, the optimized code would be:\nsevl\n1: wfe\nldxrb w2, [x0] /* Here I\u0027m saying ldaxrb (load acquire) is not required, but this is an optimization. it can just be load exclusive. */\nbic w2,w2,w1\nstlxrb w3, w2, [x0] /* this is the real change, you need this release semantic here with store release */\ncbnz w3, 1b\nret\n\nAnother sequence that is less optimized:\nsevl\n1: wee\nldaxrb w2, [x0]\nbic w2, w2, 1\nstxrb w3, w2, [x0]\ncbnz w3, 1b\ndmb /* This barrier is critical */\nret\n\nIn both the sequences above, the release semantics or barrier is critical during unlock. If you don\u0027t have that, loads and stores inside the critical section, can be reordered after the bit unlock. load-acquire does not prevent older memory ops from being reordered after it. This means, critical section memory can leak out of the critical section past the unlock.",
      "parentUuid": "8f09c5ea_77775b23",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a41678d9_bdc11039",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-20T11:40:34Z",
      "side": 1,
      "message": "also note in all cases we are using exclusive accesses to the byte with the bit locks.",
      "parentUuid": "08a9e8c4_13f5cdf3",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "261b8357_3d25b07c",
        "filename": "lib/locks/exclusive/aarch64/spinlock.S",
        "patchSetId": 3
      },
      "lineNbr": 107,
      "author": {
        "id": 1000114
      },
      "writtenOn": "2024-03-21T02:16:13Z",
      "side": 1,
      "message": "apologies. The dmb in the above less optimized sequence should be before the wfe. the dmb in my sequence above is too late, since loads and stores in critical section can reorder past the stxrb. so the less optimized sequence would be:\n\ndmb /* ensyre all memory ops in critical section complete/observed BEFORE the lock release */\nsevl\n1: wfe\nldaxrb w2, [x0]\nbic w2, w2, 1\nstern w3, w3, [x0]\ncbnz w3, 1b\nret",
      "parentUuid": "a41678d9_bdc11039",
      "range": {
        "startLine": 104,
        "startChar": 1,
        "endLine": 107,
        "endChar": 12
      },
      "revId": "76269ade963ff61240452af691692bf411536e12",
      "serverId": "8f6f209b-db1a-4cbf-aa44-c8bc30e9bfda"
    }
  ]
}